{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Web APIs & NLP\n",
    "*By: Despina Matos*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: Data Collection and Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit is an American social news aggregation, web content rating, and discussion website based off the definition on [Google](https://www.google.com/search?q=reddit&rlz=1C5CHFA_enUS877US877&oq=reddit+&aqs=chrome.0.69i59j35i39j0l2j69i60l4.4152j0j7&sourceid=chrome&ie=UTF-8). Suggested from the [Reddit's website](https://www.redditinc.com/), it has over 130,000 communities that are in the form of \"subreddits\". Each page is a platform where the Users can post, comment, and vote. A \"post\" is where the community share content by stories, links, images, and videos. A \"comment\" provides discussions on posts. And both comments & posts can be scored by being upvoted or downvoted. Yet, there is a dilemma, what if we wanted to gather data and model mulitple \"subreddits\"? This is difficult to compare such information without a classifier. Thus, **can we use supervised machine learning to classify similar content from two different web sources?**\n",
    "\n",
    "*How do we investigate this problem?* We scraped about 1000 posts from two chosen subreddits. Each subreddit we scraped was about 500 posts by using Reddit's API. Then, we finally used natural language processing to train a classifier model to check which post came for the correct subreddit. The classification models we decided to use were Logistic Regression, Bernoulli Naive Bayes, Bagged Decision Tree, and Random Forest which we evaluated on accuracy scores and results from confusion matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by pulling the data from the two subreddits by using Reddit's API. The subreddits that we pulled were the r/Books and r/Movies subreddits. The data that was imported was in JSON format. Therefore, we decided to create dataframes in Pandas to have easier access to clean and multiplate through the data.\n",
    "\n",
    "Once, we looked through the dataframes, we looked for particular subfields using the Reddit's API data dictionary. We focused on the title, author, selftext, and subreddit features. We chose these as the subfields because we wanted the best features for our modeling. \n",
    "\n",
    "Next, we did some data cleaning. We checked for duplicate posts and missing values in each of the dataframes. \n",
    "\n",
    "Then, we did some exploratory data analysis. We checked for the summary statistics and as a result, we dropped the selftext feature. Thinking back on our problem statement, we want to detemine similar content in both of the datasets, thus, we do this by looking at the frequently occurring words in each dataframe. We will did this by using an NLP functions called stemming and countvectorizer. We then chose to display bar graphs that had the top 20 frequently occurring single gram word & bigram words in each subreddit. Finally, we determined the outliers in each of our dataframes. \n",
    "\n",
    "Next, we were able to preprocess. We merged our datasets into one and dropped the author feature because we do not need it for modeling. We mapped our target variable: subreddit into a binary classification. We did some more NLP processing. We used lemmatization, stemming, and stopwords to analyize our dataset futher. Then, we created our X feature and target variable and did a train-test split. We decided to change our X feature as a stem version for our modeling. Lastly, we determined the basline score to compare to our models' results.\n",
    "\n",
    "Finally, we were able to model. We modeled four different classification models. We modeled Logistic Regression, Bernoulli Naive Bayes, Bagged Decision Tree, and Random Forest. We also created a confusion matrix for each model to have further insights on each of our models. We wanted to see how well our models were able to correctly classify where each post came from. In the end, we focused on accuracy score and the bias-variance tradeoff from each  model to determined which model was the best to answer our problem statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "- [Information on the Two Subreddits](#Information-on-the-Two-Subreddits)\n",
    "- [Data Collection](#Data-Collection)\n",
    "    - [Libraries](#Libraries)\n",
    "    - [Getting the URLs](#Getting-the-URLs) \n",
    "    - [Check the Status Codes](#Check-the-Status-Codes)\n",
    "    - [Creating JSON Objects](#Creating-JSON-Objects)\n",
    "    - [Creating Dataframes](#Creating-Dataframes)\n",
    "    - [Creating the Correct Subfields](#Creating-the-Correct-Subfields)\n",
    "- [Data Cleaning](#Data-cleaning)\n",
    "    - [Checking for Duplicate Posts](#Checking-for-Duplicate-Posts)\n",
    "    - [Checking for Missing Values](#Checking-for-Missing-Values)\n",
    "    - [Saved Clean Datasets](#Saved-Clean-Datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information on the Two Subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to choose r/Books and r/Movies as our two subreddits to answer our problem statement. Both subreddits states about anything relating to Books or Movies.\n",
    "\n",
    "The [r/Books subreddit](https://www.reddit.com/r/books/) has about 17.5 million subscribers. It was created January 25, 2008. It filters the posts by weekly thread. Also, this subreddit has eleven rules one must follow before they can post. All posts must be directly book related, informative, and discussion focused. One must use a civil tone while posting or interacting with other Users. One can not have personal request recommendations about books. One can not ask, \"What's that book called?\". One must follow the promotional rules on Reddit. One must not solicitate for pirated books. One must mark their posts that have spoliers. One can not ask for homework help. One can only post text, image, and video only. One can not post low quality book lists. And one must look at the full rules and guidelines before posting. \n",
    "\n",
    "The [r/Movies subreddit](https://www.reddit.com/r/movies/) has about 22.2 million subscribers. It was created January 25, 2008. It filters the posts by discussion, poster, media, article, trailers, news, resource, and spoliers. Also, this subreddit has eleven rules one must follow before they can post. One must watch out for self promotion when posting. One must not include hate speech in their posts. One must not include ambiguous/click-bait posts. One can not spam their posts on the site. One can not name call other Users on the site. One can not have brigading. One can not post television clips or advertisement. One can not spam post about comic book movies. Or in general, can not spam their posts on the site. One can not give lot of attention for negative things. And one can not have misleading titles with their posts. \n",
    "\n",
    "\n",
    "We believe that these subreddits are similar in content because movies are made into books. And books fans will like movies to be made from books. \n",
    "\n",
    "We should also consider in our data science process, how the pre-filtering controller in both of the subreddits sites will affect our overall resluts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We began by pulling in the necessary data by using the requests library. This library submits an HTTP requests from Python. Thus, we will be able to request our two subreddits by their specific URLs and checking their specific status codes to see if each site was successfully loaded in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling in the data \n",
    "import requests\n",
    "\n",
    "#Creating a time.sleep request\n",
    "import time\n",
    "\n",
    "#Creating a workable dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our two URLs that we have chosen are the r/Books and r/Movies sub-reddits. However, we need to use a specific type of URL to pull the data in. This type is the [Pushshift Reddit API](https://github.com/pushshift/api). Application Programming Interface (API) is the messenger that delivers our requests to Reddit and then delivers the response back to us. In other words, the API becomes like a key and askes Reddit if we can use their data for this data science process.\n",
    "\n",
    "Lastly, we need to add a size element at the end of each URL because we want to pull in 500 posts each. We chose this amount because we need to gather enough data to generate a significant result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 1: Books*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the url\n",
    "subreddit_1_url = \"https://api.pushshift.io/reddit/search/submission/?subreddit=books&size=500\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 2: Movies* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the url\n",
    "subreddit_2_url = \"https://api.pushshift.io/reddit/search/submission/?subreddit=movies&size=500\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to check the status code for each URL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Status Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We request a response or status code because we need to check if our specific HTTP request has been successfully completed. We also will need to put thought into the number of requests per second we are requesting on the Reddit's server. We do this by using the sleep method in time library. Hence, our goal is to get our status codes to be 200. \n",
    "\n",
    "For other information on other types of status codes look [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 1: Books*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make request\n",
    "res_1 = requests.get(subreddit_1_url)\n",
    "#number of requests\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the status code\n",
    "res_1.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 1 data is successfully loaded in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 2: Movies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make request\n",
    "res_2 = requests.get(subreddit_2_url)\n",
    "#number of requests\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the status code\n",
    "res_2.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 2 data is successfully loaded in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to check how our data is loaded in. We discovered that our data is in JSON formatting. *Why?* JavaScript Object Notation (JSON) is an open-standard file format. In other words, when we get data from the world wide web it is **in** JSON formatting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating JSON Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create JSON objects for each of the sub-reddits to sucessfully complete the scraping of our datasets. To clarify, we need to fully check if everything is loaded in sucessfully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 1: Books*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the json object\n",
    "results_1 = res_1.json()['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 1 data has been successfully loaded in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 2: Movies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the json object\n",
    "results_2 = res_2.json()['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 2 data has been successfully loaded in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sucessfully loaded in both of our data's subreddits. **BUT!** There is an issue, the data is not in the format where we can clean and multiplate it. \n",
    "\n",
    "In our next section, we will create dataframes to resolve this issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataframes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Pandas libary, we will be able to create two dataframes for each subreddit dataset. To emphasize, we use this to have easier access on the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 1: Books*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dataframe of subreddit 1\n",
    "subreddit_1_df = pd.DataFrame(results_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>author_premium</th>\n",
       "      <th>...</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>preview</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>thumbnail_height</th>\n",
       "      <th>thumbnail_width</th>\n",
       "      <th>suggested_sort</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>spiritofthesquirrels</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_ehnka</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>ladydadas-nightmare</td>\n",
       "      <td>reading</td>\n",
       "      <td>[{'e': 'text', 't': 'Les Mis'}]</td>\n",
       "      <td>Les Mis</td>\n",
       "      <td>richtext</td>\n",
       "      <td>t2_ttg7a</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>allahu_adamsmith</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_my89u</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>shabuluba</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_7xj3xr</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>pearloz</td>\n",
       "      <td>points-1</td>\n",
       "      <td>[{'a': ':redstar:', 'e': 'emoji', 'u': 'https:...</td>\n",
       "      <td>:redstar:11</td>\n",
       "      <td>richtext</td>\n",
       "      <td>t2_437dr</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments                author  \\\n",
       "0            []                False  spiritofthesquirrels   \n",
       "1            []                False   ladydadas-nightmare   \n",
       "2            []                False      allahu_adamsmith   \n",
       "3            []                False             shabuluba   \n",
       "4            []                False               pearloz   \n",
       "\n",
       "  author_flair_css_class                              author_flair_richtext  \\\n",
       "0                   None                                                 []   \n",
       "1                reading                    [{'e': 'text', 't': 'Les Mis'}]   \n",
       "2                   None                                                 []   \n",
       "3                   None                                                 []   \n",
       "4               points-1  [{'a': ':redstar:', 'e': 'emoji', 'u': 'https:...   \n",
       "\n",
       "  author_flair_text author_flair_type author_fullname  author_patreon_flair  \\\n",
       "0              None              text        t2_ehnka                 False   \n",
       "1           Les Mis          richtext        t2_ttg7a                 False   \n",
       "2              None              text        t2_my89u                 False   \n",
       "3              None              text       t2_7xj3xr                 False   \n",
       "4       :redstar:11          richtext        t2_437dr                 False   \n",
       "\n",
       "   author_premium  ... post_hint  preview  secure_media  secure_media_embed  \\\n",
       "0           False  ...       NaN      NaN           NaN                 NaN   \n",
       "1           False  ...       NaN      NaN           NaN                 NaN   \n",
       "2           False  ...       NaN      NaN           NaN                 NaN   \n",
       "3            True  ...       NaN      NaN           NaN                 NaN   \n",
       "4           False  ...       NaN      NaN           NaN                 NaN   \n",
       "\n",
       "  thumbnail_height thumbnail_width suggested_sort author_cakeday  \\\n",
       "0              NaN             NaN            NaN            NaN   \n",
       "1              NaN             NaN            NaN            NaN   \n",
       "2              NaN             NaN            NaN            NaN   \n",
       "3              NaN             NaN            NaN            NaN   \n",
       "4              NaN             NaN            NaN            NaN   \n",
       "\n",
       "   crosspost_parent  crosspost_parent_list  \n",
       "0               NaN                    NaN  \n",
       "1               NaN                    NaN  \n",
       "2               NaN                    NaN  \n",
       "3               NaN                    NaN  \n",
       "4               NaN                    NaN  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see if it worked\n",
    "subreddit_1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 1 data has been successfully loaded in a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check the shape of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 71)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape\n",
    "subreddit_1_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the subreddit 1, we have 500 rows and 70 columns. Each column is a different feature that a post consistent of. For instance, one of the features is the author and that feature tells us who created the post. \n",
    "\n",
    "And each of the rows are the different posts created in subreddit 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 2: Movies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dataframe of subreddit 2\n",
    "subreddit_2_df = pd.DataFrame(results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>author_premium</th>\n",
       "      <th>...</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>preview</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>thumbnail_height</th>\n",
       "      <th>thumbnail_width</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>crosspost_parent</th>\n",
       "      <th>crosspost_parent_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>topstenclub</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_534vl7cn</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>harunamika</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_crr1rvj</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>funnypilgo</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_9gwzy0x</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>westoffensive</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_31tx3ksf</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>GoldenJoel</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>t2_69bnj</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>rich:video</td>\n",
       "      <td>{'enabled': False, 'images': [{'id': 'dAeA21Yn...</td>\n",
       "      <td>{'oembed': {'author_name': 'Emory University',...</td>\n",
       "      <td>{'content': '&amp;lt;iframe width=\"600\" height=\"33...</td>\n",
       "      <td>105.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_awardings  allow_live_comments         author author_flair_css_class  \\\n",
       "0            []                False    topstenclub                   None   \n",
       "1            []                False     harunamika                   None   \n",
       "2            []                False     funnypilgo                   None   \n",
       "3            []                False  westoffensive                   None   \n",
       "4            []                False     GoldenJoel                   None   \n",
       "\n",
       "  author_flair_richtext author_flair_text author_flair_type author_fullname  \\\n",
       "0                    []              None              text     t2_534vl7cn   \n",
       "1                    []              None              text      t2_crr1rvj   \n",
       "2                    []              None              text      t2_9gwzy0x   \n",
       "3                    []              None              text     t2_31tx3ksf   \n",
       "4                    []              None              text        t2_69bnj   \n",
       "\n",
       "   author_patreon_flair  author_premium  ...   post_hint  \\\n",
       "0                 False           False  ...         NaN   \n",
       "1                 False           False  ...         NaN   \n",
       "2                 False           False  ...         NaN   \n",
       "3                 False           False  ...         NaN   \n",
       "4                 False            True  ...  rich:video   \n",
       "\n",
       "                                             preview  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  {'enabled': False, 'images': [{'id': 'dAeA21Yn...   \n",
       "\n",
       "                                        secure_media  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  {'oembed': {'author_name': 'Emory University',...   \n",
       "\n",
       "                                  secure_media_embed thumbnail_height  \\\n",
       "0                                                NaN              NaN   \n",
       "1                                                NaN              NaN   \n",
       "2                                                NaN              NaN   \n",
       "3                                                NaN              NaN   \n",
       "4  {'content': '&lt;iframe width=\"600\" height=\"33...            105.0   \n",
       "\n",
       "  thumbnail_width author_flair_background_color author_flair_text_color  \\\n",
       "0             NaN                           NaN                     NaN   \n",
       "1             NaN                           NaN                     NaN   \n",
       "2             NaN                           NaN                     NaN   \n",
       "3             NaN                           NaN                     NaN   \n",
       "4           140.0                           NaN                     NaN   \n",
       "\n",
       "   crosspost_parent  crosspost_parent_list  \n",
       "0               NaN                    NaN  \n",
       "1               NaN                    NaN  \n",
       "2               NaN                    NaN  \n",
       "3               NaN                    NaN  \n",
       "4               NaN                    NaN  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see if it worked\n",
    "subreddit_2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 2 data has been successfully loaded in a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also check the shape of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 71)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape \n",
    "subreddit_2_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 2 also has the same elements as subreddit 1. Yet, subreddit 2 has 75 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*So what is to come?* We cannot use all these features to make the best possible model for our problem statement. *How do we get the results for this then?* We need to revisit what is a classification model. A classification model is a form of supervised machine learning that predicts categorical class labels based on the training set and uses it in classifying new data. To clarify, a classficiation model has it's target variable as disrete and the features are not ordered. \n",
    "\n",
    "So in this case, we need to identify the best features we can use to have the best possible model for our problem statment. In order to get this, we need to look again at [Pushshift Reddit API](https://github.com/pushshift/api). In the next section, we will be exploring this documentation to get the correct subfields from both of our subreddit's dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Correct Subfields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this [data dictionary](https://github.com/pushshift/api), we can see which are the best features to put in a subfield. We decided on four features. \n",
    "\n",
    "We will start our subfield with the title feature because we will want to know what is the post called. Next, we will want to use the author feature because we will want to know who created the post. Then, we will want to use the selftext feature because we will like to know what is the content in the post. Finally, we will want to use the subreddit feature to determine which post was from where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the 7 features into a list\n",
    "subfield = ['title', 'author', 'selftext', 'subreddit'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement this list into our two dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 1: Books*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing the subfield in subreddit 1\n",
    "subreddit_1_df = subreddit_1_df[subfield]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Book Suggestions for my husband and I to read ...</td>\n",
       "      <td>spiritofthesquirrels</td>\n",
       "      <td>My husband and I have decided to start and fin...</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The one where Friends spoils Little Women</td>\n",
       "      <td>ladydadas-nightmare</td>\n",
       "      <td>I just started reading 'Little Women', and was...</td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The must-read works of the late, great Jim Har...</td>\n",
       "      <td>allahu_adamsmith</td>\n",
       "      <td></td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Margaret Atwood to publish first collection of...</td>\n",
       "      <td>shabuluba</td>\n",
       "      <td></td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One Man’s Impossible Quest to Read—and Review—...</td>\n",
       "      <td>pearloz</td>\n",
       "      <td></td>\n",
       "      <td>books</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                author  \\\n",
       "0  Book Suggestions for my husband and I to read ...  spiritofthesquirrels   \n",
       "1          The one where Friends spoils Little Women   ladydadas-nightmare   \n",
       "2  The must-read works of the late, great Jim Har...      allahu_adamsmith   \n",
       "3  Margaret Atwood to publish first collection of...             shabuluba   \n",
       "4  One Man’s Impossible Quest to Read—and Review—...               pearloz   \n",
       "\n",
       "                                            selftext subreddit  \n",
       "0  My husband and I have decided to start and fin...     books  \n",
       "1  I just started reading 'Little Women', and was...     books  \n",
       "2                                                        books  \n",
       "3                                                        books  \n",
       "4                                                        books  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see if it worked\n",
    "subreddit_1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 1 data has been successfully changed in our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check if our shape changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the shape\n",
    "subreddit_1_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subbreddit 1 has 500 rows and 4 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 2: Movies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing the subfield in subreddit 2\n",
    "subreddit_2_df = subreddit_2_df[subfield]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gold Jewellery Manufacturer in Jaipur</td>\n",
       "      <td>topstenclub</td>\n",
       "      <td></td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>looking for a movie similar to Stay alive, wit...</td>\n",
       "      <td>harunamika</td>\n",
       "      <td>Hello i am looking a movie similar to the movi...</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019 movies that hit you most emotionally?</td>\n",
       "      <td>funnypilgo</td>\n",
       "      <td>What movies made you cry or nearly made you bu...</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where can I watch Salò, or the 120 Days of Sod...</td>\n",
       "      <td>westoffensive</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contagion: From Simple Cough, to Global Pandemic</td>\n",
       "      <td>GoldenJoel</td>\n",
       "      <td></td>\n",
       "      <td>movies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title         author  \\\n",
       "0              Gold Jewellery Manufacturer in Jaipur    topstenclub   \n",
       "1  looking for a movie similar to Stay alive, wit...     harunamika   \n",
       "2         2019 movies that hit you most emotionally?     funnypilgo   \n",
       "3  Where can I watch Salò, or the 120 Days of Sod...  westoffensive   \n",
       "4   Contagion: From Simple Cough, to Global Pandemic     GoldenJoel   \n",
       "\n",
       "                                            selftext subreddit  \n",
       "0                                                       movies  \n",
       "1  Hello i am looking a movie similar to the movi...    movies  \n",
       "2  What movies made you cry or nearly made you bu...    movies  \n",
       "3                                          [removed]    movies  \n",
       "4                                                       movies  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see if it worked\n",
    "subreddit_2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 2 data has been successfully changed in our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also check if our shape changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the shape\n",
    "subreddit_2_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 2 has 500 rows and 4 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start our data cleaning on both dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to generate a great model, we need to clean up our datasets before we use them. We need to check for duplicate posts and check for missing values in our dataframes so we will not skewed our model results. Finally, we can save our clean dataframes to use for the rest of our data science process in a different notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Duplicate Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check for duplicate posts in both of our dataframes and if we do have them, lets drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 1: Books*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking and dropping the duplicates in subreddit 1 \n",
    "subreddit_1_df = subreddit_1_df.drop_duplicates() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see if it worked, lets look at the shape\n",
    "subreddit_1_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 1 now has 492 rows and 4 columns. So there must of been eight duplicate posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 2: Movies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking and dropping the duplicates in subreddit 2\n",
    "subreddit_2_df = subreddit_2_df.drop_duplicates() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(475, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check to see if it worked, lets look at the shape\n",
    "subreddit_2_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 2 now has 475 rows and 4 columns. So there must of been 25 duplicate posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should check for missing data in each dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check for the missing values in our dataframes because we will ideally want no missing values. However, if we do, then we will need to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 1: Books*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        0\n",
       "author       0\n",
       "selftext     0\n",
       "subreddit    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using isnull with sum to check for missing values\n",
    "subreddit_1_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 1 does not have any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 2: Movies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title        0\n",
       "author       0\n",
       "selftext     0\n",
       "subreddit    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using isnull with sum to check for missing values\n",
    "subreddit_2_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our subreddit 2 does not have any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, our datasets do not have to be fixed. \n",
    "\n",
    "Let's save our dataframes so we can work with them in later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved Clean Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 1: Books*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data is clean so far in subreddit 1, here is what we will like to save it as\n",
    "#index = false for no index column\n",
    "subreddit_1 = subreddit_1_df.to_csv('../datasets/subreddit1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subreddit 1: Movies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data is clean so far in subreddit 2, here is what we will like to save it as\n",
    "#index = false for no index column\n",
    "subreddit_2 = subreddit_2_df.to_csv('../datasets/subreddit2.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
